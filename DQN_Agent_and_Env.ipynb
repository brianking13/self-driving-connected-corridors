{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc143af",
   "metadata": {},
   "source": [
    "# Please Install these Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5125e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.5.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (1.34.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (1.12.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (1.19.5)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (0.15.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (1.12)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (3.19.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (0.37.1)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (2.10.1)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (65.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.33.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.26.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.2.1)\n",
      "Requirement already satisfied: gym in c:\\users\\bking\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from gym) (1.19.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\bking\\anaconda3\\lib\\site-packages (from gym) (1.7.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in c:\\users\\bking\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: keras in c:\\users\\bking\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: keras-rl2 in c:\\users\\bking\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\bking\\anaconda3\\lib\\site-packages (from keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (0.37.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.19.5)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (0.15.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (0.4.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.34.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (3.19.1)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (2.10.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.12.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (3.1.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (2.27.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (65.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\bking\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.5.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd601d",
   "metadata": {},
   "source": [
    "#  Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12aa2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef078326",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e84ff",
   "metadata": {},
   "source": [
    "### The Vehicle Class contains all the information for the DQN Agent (step, reset, rewards, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "814f5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehEnv(Env):\n",
    "    def __init__(self,visualize,traffic_lights,mass,frontal_area,air_density,drag_cof,Crr):\n",
    "        self.traffic_lights = traffic_lights\n",
    "        self.mass = mass\n",
    "        self.frontal_area = frontal_area\n",
    "        self.air_density = air_density\n",
    "        self.drag_cof = drag_cof\n",
    "        self.Crr = Crr\n",
    "   \n",
    "        \n",
    "        # number of actions (accelerations)- Discrete is discrete set of values\n",
    "        self.action_space = Discrete(11)                         \n",
    "        # upper and lower bounds for speed- Box is continuous set of values- normalize?\n",
    "        self.observation_space = Box(low=np.array([0.0, 0.0, 0.0,0]), high = np.array([40.0/40.0, 1000/1000,40/40,1])) \n",
    "        \n",
    "        # parameters for normilization\n",
    "        self.speed_max = 40\n",
    "        self.dist_max = 1000\n",
    "        self.time_max = 40\n",
    "        \n",
    "        \n",
    "        # start parameters\n",
    "        self.power = 0\n",
    "        self.energy = 0\n",
    "        self.traffic_lights.reset()\n",
    "        next_pos = self.traffic_lights.get_info()[0][0]\n",
    "        self.prev_light_pos = self.traffic_lights.get_info()[0][0]\n",
    "        next_phase = self.traffic_lights.get_info()[0][3]\n",
    "        if next_phase == 0:\n",
    "             next_time = self.traffic_lights.get_info()[0][2]\n",
    "        else:\n",
    "            next_time = self.traffic_lights.get_info()[0][1]\n",
    "        self.x = 0 \n",
    "        self.state = [self.normalize(self.speed_max,random.randint(0,35)), self.normalize(self.dist_max,next_pos),self.normalize(self.time_max,next_time), next_phase]\n",
    "\n",
    "        # set simulation length\n",
    "        self.sim_length = 1000\n",
    "        \n",
    "        self.vis = visualize\n",
    "        if self.vis:\n",
    "            global speeds \n",
    "            global accel\n",
    "            global light_vis_info\n",
    "            global total_energy\n",
    "            speeds = []\n",
    "            accels = []\n",
    "            position = []\n",
    "            light_vis_info = []\n",
    "            total_energy = []\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def step(self,action):\n",
    "        reward = 0\n",
    "        \n",
    "        # map discrete values to discrete acceleration values  between -3 and 2 (y=0.5x-2)\n",
    "        accel = 0.5*action-3\n",
    "        # get previous state\n",
    "        prev_speed = (self.state[0]*self.speed_max/2)+self.speed_max/2\n",
    "        prev_x = self.x\n",
    "        # get new speed and position and SPaT data\n",
    "        speed = prev_speed+ accel\n",
    "        self.x = prev_x + prev_speed+(speed-prev_speed)/2\n",
    "        \n",
    "        # collect data for ran red function\n",
    "        prev_phase = self.state[3] # previous phase \n",
    "\n",
    "        # change traffic lights\n",
    "        self.traffic_lights.step()\n",
    "        \n",
    "        # calculate energy\n",
    "        [energy,power] = self.power_calcs(accel,speed)\n",
    "        self.energy += energy\n",
    "        self.power = power\n",
    "        \n",
    "        # drop simulation time every time step\n",
    "        self.sim_length -= 1\n",
    "        \n",
    "        # check next light info\n",
    "        y = self.check_next_light(self.x)\n",
    "        next_light_pos = self.traffic_lights.get_info()[y][0]\n",
    "        next_pos = next_light_pos-self.x\n",
    "        next_phase = self.traffic_lights.get_info()[y][3]\n",
    "        if next_phase == 0:\n",
    "             next_time = self.traffic_lights.get_info()[y][2]\n",
    "        else:\n",
    "            next_time = self.traffic_lights.get_info()[y][1]\n",
    "        \n",
    "        # check if light was ran\n",
    "        if self.prev_light_pos !=  next_light_pos:\n",
    "            reward += self.ran_red(prev_phase)\n",
    "    \n",
    "        # only do this if visualization is necessary \n",
    "        if self.vis:\n",
    "            speeds.append(speed)\n",
    "            accels.append(accel)\n",
    "            positions.append(self.x)\n",
    "            total_energy.append(self.energy)\n",
    "            info = self.traffic_lights.get_info()\n",
    "            lights_at_time_step = []\n",
    "            for i in range(0,len(info)):   \n",
    "                lights_at_time_step.append([info[i][0],info[i][3]])\n",
    "            light_vis_info.append(lights_at_time_step)\n",
    "        else:\n",
    "            pass     \n",
    "        \n",
    "        # reward function\n",
    "#         error = (speed-25)**2\n",
    "#         reward = reward + -2/(1+2.7**(-0.05*error))+2\n",
    "        reward = reward + 1/(2.7**(0.5*accel**2))\n",
    "        \n",
    "    \n",
    "        # check if simulation is done\n",
    "        last_light = self.traffic_lights.get_info()[len(self.traffic_lights.get_info())-1][0]\n",
    "        if self.sim_length <= 0 or speed < 0 or self.x >= last_light or reward < 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "    \n",
    "        # return step information\n",
    "        self.prev_light_pos = next_light_pos\n",
    "        self.state = [self.normalize(self.speed_max,speed), self.normalize(self.dist_max,next_pos),self.normalize(self.time_max,next_time), next_phase]\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "    def ran_red(self,prev_phase):\n",
    "        # the light could turn green or red at an interval we don't check... this will be fixed later\n",
    "        # the light will only check at the last phase before vehicle passes through\n",
    "        if prev_phase == 1:\n",
    "            # did not ran red\n",
    "            return 10\n",
    "        elif prev_phase == 0:\n",
    "            # ran red\n",
    "            return -10\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def power_calcs(self,accel,speed):\n",
    "        drag = 0.5*self.drag_cof*self.frontal_area*self.air_density*speed**2\n",
    "        rolling_res = mass*9.81*Crr ###Fix\n",
    "        force = self.mass*accel + drag + rolling_res\n",
    "        power = force*speed/1000 # convert to kW\n",
    "        if power < 0:\n",
    "            power = 0\n",
    "        \n",
    "        energy = self.energy+self.power+(power-self.power)/2\n",
    "        energy = energy*0.000277778 # convert from kJ to kWh\n",
    "        return energy, power\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def check_next_light(self,x):\n",
    "        c = self.traffic_lights.get_info()\n",
    "        if x < c[0][0]:\n",
    "            y = 0\n",
    "        if x > c[len(c)-1][0]:\n",
    "            y = len(c)-1\n",
    "        else:\n",
    "            for i in range(0,len(c)-1):\n",
    "                if x >= c[i][0] and x <= c[i+1][0]:\n",
    "                    y = i + 1\n",
    "        return y\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def normalize(self,max,x):\n",
    "        normed = (x-max/2)/(max/2)\n",
    "        return normed\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "#         reset initial conditions\n",
    "        self.power = 0\n",
    "        self.energy = 0\n",
    "\n",
    "        self.traffic_lights.reset()\n",
    "        next_pos = self.traffic_lights.get_info()[0][0]\n",
    "        next_phase = self.traffic_lights.get_info()[0][3]\n",
    "        if next_phase == 0:\n",
    "             next_time = self.traffic_lights.get_info()[0][2]\n",
    "        else:\n",
    "            next_time = self.traffic_lights.get_info()[0][1]\n",
    "        self.x = 0\n",
    "        self.state = [self.normalize(self.speed_max,random.randint(0,35)), self.normalize(self.dist_max,next_pos),self.normalize(self.time_max,next_time), next_phase]\n",
    "        \n",
    "        # reset time\n",
    "        self.sim_length = 1000\n",
    "        if self.vis:\n",
    "            speeds = []\n",
    "            accels = []\n",
    "            positions = []\n",
    "            light_vis_info = []\n",
    "            total_energy = []\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea76fe89",
   "metadata": {},
   "source": [
    "### This Traffic Light Class creates a single Traffic Light with Signal Phasing and Timing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b20a7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficLight():\n",
    "    def __init__(self,position,green_time,red_time):\n",
    "        # green is 1, red is 0\n",
    "        self.initial_green_time = green_time\n",
    "        self.initial_red_time = red_time\n",
    "        self.initial_position = position\n",
    "        self.position = position\n",
    "        self.green_time = green_time\n",
    "        self.red_time = red_time\n",
    "        self.phase =random.randint(0,1)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.position = self.initial_position\n",
    "        self.green_time = self.initial_green_time\n",
    "        self.red_time = self.initial_red_time\n",
    "        self.phase = random.randint(0,1)\n",
    "\n",
    "    def step(self):\n",
    "        if self.phase == 1:\n",
    "            self.green_time -= 1\n",
    "        elif self.phase == 0:\n",
    "            self.red_time -= 1\n",
    "\n",
    "        if self.green_time <= 0:\n",
    "            self.phase = 0\n",
    "            self.green_time = self.initial_green_time\n",
    "        elif self.red_time <= 0:\n",
    "            self.phase = 1\n",
    "            self.red_time = self.initial_red_time\n",
    "        return None\n",
    "    \n",
    "    def get_info(self): \n",
    "        return [self.position, self.green_time, self.red_time,self.phase]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b33089",
   "metadata": {},
   "source": [
    "### This Corridor Class builds an entire Street of Lights using the previous Traffic Light Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "087989c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corridor():\n",
    "    def __init__(self,*args):\n",
    "        self.y = [x for x in args]\n",
    "        \n",
    "    def step(self):\n",
    "        for x in self.y:\n",
    "            x.step()\n",
    "            \n",
    "    def reset(self):\n",
    "        for x in self.y:\n",
    "            x.reset()\n",
    "        \n",
    "    def get_info(self):\n",
    "        return [x.get_info() for x in self.y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1698e10",
   "metadata": {},
   "source": [
    "# Vehicle Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ec2fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## user input\n",
    "# mass = input('Vehicle Mass in kg: ')\n",
    "# frontal_area = input('Frontal area of vehicle in m^2: ')\n",
    "# air_density =  input('Density of air in kg/m^3: ')\n",
    "# drag_cof = input('Drag Coefficient: ')\n",
    "\n",
    "# pre-defined\n",
    "mass = 2050 \n",
    "frontal_area = 2\n",
    "air_density = 1.2\n",
    "drag_cof = .38\n",
    "Crr = .01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ff520",
   "metadata": {},
   "source": [
    "# Test that Traffic Light and Corridor Classes are functioning properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "304764af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Test light class\n",
    "# TL = TrafficLight(20,30,25)    # position, green_time, red_time\n",
    "# for i in range(1,920):\n",
    "#     TL.step()\n",
    "# print(TL.get_info())\n",
    "# TL1 = TrafficLight(20,3,25) \n",
    "# TL2 = TrafficLight(20,30,25) \n",
    "# TL3 = TrafficLight(20,40,25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f70271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test corridor class\n",
    "# c = Corridor(TL1,TL2,TL3)\n",
    "# print(c.get_info())\n",
    "# for i in range(1,20):\n",
    "#     c.step()\n",
    "# print(c.get_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98df36",
   "metadata": {},
   "source": [
    "# Build Traffic Light Corridor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20d3dc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bking\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1000, 25, 40, 1],\n",
       " [2200, 35, 30, 1],\n",
       " [2600, 40, 30, 1],\n",
       " [3000, 35, 40, 0],\n",
       " [5000, 35, 40, 0],\n",
       " [6500, 35, 40, 0],\n",
       " [7000, 35, 40, 0],\n",
       " [7900, 25, 40, 1],\n",
       " [8200, 35, 30, 1],\n",
       " [9000, 40, 30, 1],\n",
       " [9900, 35, 40, 0],\n",
       " [10200, 35, 40, 1],\n",
       " [11000, 35, 40, 1],\n",
       " [14100, 35, 40, 0],\n",
       " [16000, 35, 40, 1],\n",
       " [17200, 35, 40, 1]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build traffic light corridor\n",
    "TL1 = TrafficLight(1000,25,40) \n",
    "TL2 = TrafficLight(2200,35,30) \n",
    "TL3 = TrafficLight(2600,40,30) \n",
    "TL4 = TrafficLight(3000,35,40)\n",
    "TL5 = TrafficLight(5000,35,40)\n",
    "TL6 = TrafficLight(6500,35,40)\n",
    "TL7 = TrafficLight(7000,35,40)\n",
    "\n",
    "TL8 = TrafficLight(7900,25,40) \n",
    "TL9 = TrafficLight(8200,35,30) \n",
    "TL10 = TrafficLight(9000,40,30) \n",
    "TL11= TrafficLight(9900,35,40)\n",
    "TL12= TrafficLight(10200,35,40)\n",
    "TL13= TrafficLight(11000,35,40)\n",
    "TL14= TrafficLight(14100,35,40)\n",
    "TL15= TrafficLight(16000,35,40)\n",
    "TL16= TrafficLight(17200,35,40)\n",
    "\n",
    "corridor = Corridor(TL1,TL2,TL3,TL4,TL5,TL6,TL7,TL8,TL9,TL10,TL11,TL12,TL13,TL14,TL15,TL16)\n",
    "# build environment\n",
    "env = VehEnv(False, corridor,mass,frontal_area,air_density,drag_cof,Crr)\n",
    "corridor.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce44d4",
   "metadata": {},
   "source": [
    "## Verify environment is returning appropriate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6541e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5258327 , 0.5750535 , 0.486482  , 0.57344747], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a7c301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing car matches with matlab and other Python model\n",
    "# state = env.reset()\n",
    "# done = False\n",
    "# score = 0\n",
    "# for i in range(1,21):\n",
    "#     action = env.step(10)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "535722ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65, 0.934, 0.2, 1]\n",
      "[0.75, 0.866, 0.15, 1]\n",
      "[0.775, 0.7955, 0.1, 1]\n",
      "[0.7, 0.726, 0.05, 1]\n",
      "[0.675, 0.6585, 0.0, 1]\n",
      "[0.7, 0.591, -0.05, 1]\n",
      "[0.65, 0.524, -0.1, 1]\n",
      "[0.625, 0.4585, -0.15, 1]\n",
      "[0.575, 0.3945, -0.2, 1]\n",
      "[0.525, 0.3325, -0.25, 1]\n",
      "[0.525, 0.2715, -0.3, 1]\n",
      "[0.55, 0.21, -0.35, 1]\n",
      "[0.425, 0.1505, -0.4, 1]\n",
      "[0.4, 0.094, -0.45, 1]\n",
      "[0.3, 0.04, -0.5, 1]\n",
      "[0.375, -0.0135, -0.55, 1]\n",
      "[0.375, -0.0685, -0.6, 1]\n",
      "[0.45, -0.125, -0.65, 1]\n",
      "[0.45, -0.183, -0.7, 1]\n",
      "[0.425, -0.2405, -0.75, 1]\n",
      "[0.3, -0.295, -0.8, 1]\n",
      "[0.325, -0.3475, -0.85, 1]\n",
      "[0.375, -0.4015, -0.9, 1]\n",
      "[0.325, -0.4555, -0.95, 1]\n",
      "[0.325, -0.5085, 1.0, 0]\n",
      "[0.4, -0.563, 0.95, 0]\n",
      "[0.45, -0.62, 0.9, 0]\n",
      "[0.4, -0.677, 0.85, 0]\n",
      "[0.4, -0.733, 0.8, 0]\n",
      "[0.3, -0.787, 0.75, 0]\n",
      "[0.25, -0.838, 0.7, 0]\n",
      "[0.275, -0.8885, 0.65, 0]\n",
      "[0.35, -0.941, 0.6, 0]\n",
      "[0.275, -0.9935, 0.55, 0]\n",
      "[0.15, 1.358, 0.5, 1]\n",
      "Episode:1 Score:11.326721383321917\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "for episode in range(1,episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()  \n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+= reward\n",
    "        print(n_state)\n",
    "    print('Episode:{} Score:{}'.format(episode,score))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcdc592",
   "metadata": {},
   "source": [
    "# Build DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f631e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d910527b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      2\u001b[0m actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states[0])))\n",
    "    model.add(Dense(128, activation='relu', input_shape = (1,states[0])))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(actions, activation ='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fef1ef",
   "metadata": {},
   "source": [
    "# Build Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4beb51",
   "metadata": {},
   "source": [
    "### Load a Saved Model (you can use 'model-acceleration_based' from Github or your own trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59eb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model-acceleration_based')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32facecd",
   "metadata": {},
   "source": [
    "### Build and compile agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f5c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, acitons):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=100000, window_length =1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy= policy, nb_actions=actions, nb_steps_warmup=10, target_model_update = 1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ffffb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bking\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594357c",
   "metadata": {},
   "source": [
    "### Create a New Model (do not run if simply using the loaded saved model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16ab667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "\r",
      "    1/10000 [..............................] - ETA: 8:40 - reward: 10.8832"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bking\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "C:\\Users\\bking\\anaconda3\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 36s 4ms/step - reward: 0.5888\n",
      "110 episodes - episode_reward: 51.293 [-9.989, 338.662] - loss: 9.549 - mae: 37.454 - mean_q: 43.568\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 0.6367\n",
      "59 episodes - episode_reward: 108.590 [-9.863, 509.140] - loss: 23.654 - mae: 82.264 - mean_q: 93.743\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 0.7193\n",
      "21 episodes - episode_reward: 337.529 [-9.863, 757.477] - loss: 27.500 - mae: 106.177 - mean_q: 120.325\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 0.7268\n",
      "19 episodes - episode_reward: 391.986 [-9.117, 754.296] - loss: 25.463 - mae: 125.024 - mean_q: 141.351\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 0.7357\n",
      "13 episodes - episode_reward: 548.149 [-9.391, 746.021] - loss: 27.575 - mae: 138.839 - mean_q: 156.525\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 0.7351\n",
      "15 episodes - episode_reward: 484.351 [-9.117, 793.770] - loss: 27.177 - mae: 143.673 - mean_q: 161.835\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 0.7334\n",
      "17 episodes - episode_reward: 430.823 [-9.863, 790.303] - loss: 24.757 - mae: 142.932 - mean_q: 160.664\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 47s 5ms/step - reward: 0.7416\n",
      "14 episodes - episode_reward: 525.181 [-9.117, 767.280] - loss: 22.315 - mae: 140.217 - mean_q: 157.507\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: 0.7318\n",
      "14 episodes - episode_reward: 514.062 [-9.117, 766.694] - loss: 20.031 - mae: 136.264 - mean_q: 152.917\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: 0.7259\n",
      "16 episodes - episode_reward: 455.204 [-9.863, 760.720] - loss: 18.497 - mae: 134.457 - mean_q: 150.875\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7335\n",
      "17 episodes - episode_reward: 463.619 [-9.863, 764.292] - loss: 17.256 - mae: 133.789 - mean_q: 149.884\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7086\n",
      "24 episodes - episode_reward: 272.772 [-9.863, 719.283] - loss: 17.671 - mae: 134.951 - mean_q: 150.975\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7341\n",
      "26 episodes - episode_reward: 292.456 [-9.863, 763.453] - loss: 18.854 - mae: 135.538 - mean_q: 151.397\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7445\n",
      "25 episodes - episode_reward: 291.240 [-9.391, 758.439] - loss: 16.280 - mae: 130.616 - mean_q: 145.904\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7292\n",
      "16 episodes - episode_reward: 445.381 [-9.117, 754.295] - loss: 15.533 - mae: 126.472 - mean_q: 141.219\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7205\n",
      "17 episodes - episode_reward: 455.639 [-9.673, 748.108] - loss: 14.135 - mae: 124.216 - mean_q: 138.693\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7297\n",
      "27 episodes - episode_reward: 273.765 [-9.863, 760.073] - loss: 16.246 - mae: 122.885 - mean_q: 137.330\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.7297\n",
      "20 episodes - episode_reward: 358.720 [-9.391, 765.383] - loss: 15.149 - mae: 122.100 - mean_q: 136.347\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.7207\n",
      "24 episodes - episode_reward: 300.134 [-9.863, 752.211] - loss: 14.625 - mae: 121.596 - mean_q: 135.931\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.7304\n",
      "26 episodes - episode_reward: 283.564 [-9.863, 717.638] - loss: 17.014 - mae: 123.819 - mean_q: 138.533\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7359\n",
      "22 episodes - episode_reward: 337.752 [-9.673, 752.228] - loss: 18.987 - mae: 125.303 - mean_q: 140.142\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7350\n",
      "15 episodes - episode_reward: 479.345 [-9.673, 759.808] - loss: 18.182 - mae: 125.991 - mean_q: 140.883\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 0.7251\n",
      "14 episodes - episode_reward: 500.998 [-9.673, 741.028] - loss: 16.857 - mae: 126.078 - mean_q: 140.898\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7259\n",
      "20 episodes - episode_reward: 358.353 [-9.673, 738.381] - loss: 15.741 - mae: 125.279 - mean_q: 139.982\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7229\n",
      "23 episodes - episode_reward: 310.052 [-9.673, 753.285] - loss: 16.215 - mae: 124.364 - mean_q: 138.935\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.7233\n",
      "17 episodes - episode_reward: 459.055 [-9.673, 737.446] - loss: 15.274 - mae: 122.852 - mean_q: 137.236\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.7388\n",
      "16 episodes - episode_reward: 448.044 [-9.673, 749.304] - loss: 15.200 - mae: 123.402 - mean_q: 137.786\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: 0.7274\n",
      "21 episodes - episode_reward: 348.241 [-9.863, 747.465] - loss: 15.868 - mae: 122.306 - mean_q: 136.587\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 0.7270\n",
      "16 episodes - episode_reward: 442.098 [-9.673, 765.850] - loss: 15.489 - mae: 121.974 - mean_q: 136.126\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7437\n",
      "24 episodes - episode_reward: 324.078 [-9.673, 757.235] - loss: 14.979 - mae: 120.742 - mean_q: 134.727\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7351\n",
      "17 episodes - episode_reward: 403.200 [-9.117, 755.585] - loss: 15.130 - mae: 121.319 - mean_q: 135.458\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: 0.7272\n",
      "22 episodes - episode_reward: 354.611 [-9.863, 744.017] - loss: 15.225 - mae: 119.681 - mean_q: 133.508\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: 0.7198\n",
      "16 episodes - episode_reward: 445.071 [-9.117, 751.878] - loss: 14.113 - mae: 119.739 - mean_q: 133.524\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.7307\n",
      "27 episodes - episode_reward: 267.287 [-9.955, 741.511] - loss: 15.832 - mae: 121.667 - mean_q: 135.837\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.7266\n",
      "19 episodes - episode_reward: 360.878 [-9.391, 727.401] - loss: 15.588 - mae: 122.706 - mean_q: 137.010\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 0.7180\n",
      "21 episodes - episode_reward: 370.697 [-9.391, 741.473] - loss: 14.836 - mae: 121.364 - mean_q: 135.494\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7314\n",
      "19 episodes - episode_reward: 364.596 [-9.955, 763.259] - loss: 14.808 - mae: 121.173 - mean_q: 135.315\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.7306\n",
      "18 episodes - episode_reward: 419.886 [-9.391, 745.677] - loss: 15.796 - mae: 121.676 - mean_q: 135.940\n",
      "\n",
      "Interval 39 (380000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.7154\n",
      "18 episodes - episode_reward: 370.067 [-9.391, 747.156] - loss: 15.291 - mae: 121.223 - mean_q: 135.255\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 0.7013\n",
      "18 episodes - episode_reward: 391.295 [-9.391, 769.175] - loss: 14.766 - mae: 120.566 - mean_q: 134.554\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.7219\n",
      "18 episodes - episode_reward: 401.651 [-9.955, 745.074] - loss: 13.973 - mae: 119.985 - mean_q: 133.888\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7187\n",
      "20 episodes - episode_reward: 377.622 [-9.955, 751.240] - loss: 14.124 - mae: 118.537 - mean_q: 132.284\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.7277\n",
      "21 episodes - episode_reward: 349.770 [-9.989, 734.457] - loss: 13.893 - mae: 118.685 - mean_q: 132.491\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 0.7120\n",
      "22 episodes - episode_reward: 307.313 [-9.391, 731.612] - loss: 13.965 - mae: 118.125 - mean_q: 131.849\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.7207\n",
      "21 episodes - episode_reward: 355.733 [-9.673, 732.417] - loss: 13.356 - mae: 118.926 - mean_q: 132.610\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 0.7313\n",
      "18 episodes - episode_reward: 390.196 [-9.989, 749.641] - loss: 13.092 - mae: 119.605 - mean_q: 133.456\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7144\n",
      "21 episodes - episode_reward: 356.232 [-9.989, 758.985] - loss: 13.753 - mae: 118.969 - mean_q: 132.759\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: 0.7127\n",
      "18 episodes - episode_reward: 405.743 [-9.673, 739.532] - loss: 13.196 - mae: 118.871 - mean_q: 132.594\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.7162\n",
      "15 episodes - episode_reward: 475.761 [-9.673, 755.736] - loss: 12.076 - mae: 117.703 - mean_q: 131.156\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 0.6998\n",
      "18 episodes - episode_reward: 388.770 [-9.673, 723.243] - loss: 12.500 - mae: 116.337 - mean_q: 129.654\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.6960\n",
      "28 episodes - episode_reward: 250.212 [-9.673, 733.149] - loss: 12.814 - mae: 115.300 - mean_q: 128.510\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 0.7080\n",
      "15 episodes - episode_reward: 448.771 [-9.673, 733.024] - loss: 13.112 - mae: 114.725 - mean_q: 127.982\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.7122\n",
      "23 episodes - episode_reward: 302.083 [-9.673, 723.717] - loss: 13.195 - mae: 116.630 - mean_q: 130.111\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.7177\n",
      "17 episodes - episode_reward: 448.082 [-9.117, 726.089] - loss: 13.447 - mae: 119.964 - mean_q: 133.774\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.7309\n",
      "22 episodes - episode_reward: 328.355 [-9.673, 744.320] - loss: 14.385 - mae: 119.677 - mean_q: 133.505\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.7127\n",
      "21 episodes - episode_reward: 342.415 [-9.673, 742.640] - loss: 12.673 - mae: 117.599 - mean_q: 131.060\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 0.7189\n",
      "27 episodes - episode_reward: 263.197 [-9.673, 696.672] - loss: 13.304 - mae: 117.411 - mean_q: 130.991\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7131\n",
      "18 episodes - episode_reward: 389.019 [-9.673, 739.313] - loss: 13.762 - mae: 118.350 - mean_q: 131.958\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.7080\n",
      "25 episodes - episode_reward: 267.898 [-9.673, 740.063] - loss: 14.121 - mae: 118.806 - mean_q: 132.477\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.7137\n",
      "24 episodes - episode_reward: 306.609 [-9.673, 750.901] - loss: 15.048 - mae: 119.296 - mean_q: 133.113\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.7241\n",
      "25 episodes - episode_reward: 291.372 [-9.117, 740.655] - loss: 15.442 - mae: 120.230 - mean_q: 134.126\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.7321\n",
      "21 episodes - episode_reward: 345.465 [-9.673, 748.560] - loss: 15.574 - mae: 122.319 - mean_q: 136.418\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.7274\n",
      "22 episodes - episode_reward: 349.545 [-9.863, 742.854] - loss: 15.532 - mae: 123.330 - mean_q: 137.539\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7213\n",
      "21 episodes - episode_reward: 326.105 [-9.863, 741.031] - loss: 17.565 - mae: 122.962 - mean_q: 137.136\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: 0.7111\n",
      "19 episodes - episode_reward: 392.396 [-9.863, 732.059] - loss: 16.216 - mae: 122.204 - mean_q: 136.374\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 0.7113\n",
      "25 episodes - episode_reward: 278.369 [-9.989, 729.700] - loss: 17.097 - mae: 123.072 - mean_q: 137.425\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.7217\n",
      "14 episodes - episode_reward: 486.347 [-9.117, 734.985] - loss: 17.895 - mae: 125.391 - mean_q: 139.990\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.7269\n",
      "20 episodes - episode_reward: 388.341 [-9.989, 726.907] - loss: 16.440 - mae: 125.081 - mean_q: 139.584\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: 0.7306\n",
      "18 episodes - episode_reward: 378.385 [-9.989, 750.200] - loss: 16.634 - mae: 123.924 - mean_q: 138.214\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.7244\n",
      "20 episodes - episode_reward: 382.160 [-9.989, 725.900] - loss: 16.168 - mae: 123.673 - mean_q: 138.016\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7173\n",
      "25 episodes - episode_reward: 298.320 [-9.989, 736.134] - loss: 16.256 - mae: 123.304 - mean_q: 137.474\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.7321\n",
      "16 episodes - episode_reward: 427.222 [-9.955, 738.828] - loss: 15.249 - mae: 121.458 - mean_q: 135.511\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.7021\n",
      "28 episodes - episode_reward: 253.784 [-9.955, 727.567] - loss: 17.193 - mae: 123.216 - mean_q: 137.646\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7173\n",
      "28 episodes - episode_reward: 268.911 [-9.989, 715.340] - loss: 22.709 - mae: 135.573 - mean_q: 151.379\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.6976\n",
      "21 episodes - episode_reward: 310.962 [-9.989, 720.923] - loss: 19.677 - mae: 134.524 - mean_q: 150.036\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7121\n",
      "15 episodes - episode_reward: 497.881 [-9.955, 728.645] - loss: 18.234 - mae: 129.543 - mean_q: 144.392\n",
      "\n",
      "Interval 77 (760000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.7194\n",
      "17 episodes - episode_reward: 391.846 [-9.989, 726.269] - loss: 17.310 - mae: 125.718 - mean_q: 140.260\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7089\n",
      "18 episodes - episode_reward: 416.836 [-9.117, 732.238] - loss: 17.780 - mae: 126.309 - mean_q: 140.992\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.7024\n",
      "22 episodes - episode_reward: 322.952 [-9.117, 729.073] - loss: 17.005 - mae: 125.956 - mean_q: 140.544\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: 0.7258\n",
      "15 episodes - episode_reward: 483.665 [-9.955, 741.438] - loss: 16.976 - mae: 124.438 - mean_q: 138.767\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7186\n",
      "18 episodes - episode_reward: 406.287 [-9.117, 724.808] - loss: 16.165 - mae: 122.501 - mean_q: 136.799\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7230\n",
      "17 episodes - episode_reward: 418.859 [-9.989, 747.349] - loss: 15.932 - mae: 120.639 - mean_q: 134.677\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.7227\n",
      "24 episodes - episode_reward: 296.818 [-9.117, 762.796] - loss: 14.716 - mae: 120.812 - mean_q: 135.007\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7319\n",
      "18 episodes - episode_reward: 399.112 [-9.117, 729.968] - loss: 13.351 - mae: 121.108 - mean_q: 135.243\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7336\n",
      "22 episodes - episode_reward: 343.717 [-9.863, 734.589] - loss: 15.260 - mae: 122.188 - mean_q: 136.544\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7110\n",
      "18 episodes - episode_reward: 402.290 [-9.673, 736.415] - loss: 15.889 - mae: 123.474 - mean_q: 137.845\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7097\n",
      "21 episodes - episode_reward: 329.665 [-9.989, 724.602] - loss: 14.579 - mae: 122.606 - mean_q: 136.948\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7169\n",
      "18 episodes - episode_reward: 374.366 [-9.989, 724.480] - loss: 14.513 - mae: 121.808 - mean_q: 136.126\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7177\n",
      "17 episodes - episode_reward: 427.495 [-9.989, 734.166] - loss: 13.947 - mae: 120.446 - mean_q: 134.662\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.6982\n",
      "26 episodes - episode_reward: 285.814 [-9.673, 727.458] - loss: 14.666 - mae: 120.423 - mean_q: 134.743\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.7075\n",
      "19 episodes - episode_reward: 367.135 [-9.673, 736.052] - loss: 14.779 - mae: 120.699 - mean_q: 134.994\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7061\n",
      "21 episodes - episode_reward: 344.855 [-9.673, 736.203] - loss: 14.619 - mae: 119.545 - mean_q: 133.907\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.7254\n",
      "19 episodes - episode_reward: 352.003 [-9.673, 751.507] - loss: 17.128 - mae: 124.049 - mean_q: 139.072\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.7097\n",
      "18 episodes - episode_reward: 400.436 [-9.673, 742.659] - loss: 15.051 - mae: 124.106 - mean_q: 139.096\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.7217\n",
      "19 episodes - episode_reward: 398.334 [-9.673, 733.403] - loss: 15.841 - mae: 123.484 - mean_q: 138.590\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7243\n",
      "18 episodes - episode_reward: 385.525 [-9.863, 726.390] - loss: 15.223 - mae: 124.079 - mean_q: 139.308\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: 0.7149\n",
      "25 episodes - episode_reward: 282.220 [-9.863, 753.962] - loss: 15.711 - mae: 123.317 - mean_q: 138.249\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7431\n",
      "27 episodes - episode_reward: 282.817 [-9.863, 717.851] - loss: 15.656 - mae: 124.073 - mean_q: 138.979\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7195\n",
      "15 episodes - episode_reward: 483.083 [-9.863, 730.317] - loss: 17.227 - mae: 125.859 - mean_q: 141.246\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: 0.7183\n",
      "done, took 5318.901 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps =1000000, visualize=False, verbose =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e71278",
   "metadata": {},
   "source": [
    "### Save Model to current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98f31cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model-acceleration_based_fuel_efficient_keras.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3629736",
   "metadata": {},
   "source": [
    "### Show training stats and neural network weights/biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79a2f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dec84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "%matplotlib qt \n",
    "plt.plot(history.history['episode_reward'])\n",
    "plt.ylabel('reward')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "# model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371849df",
   "metadata": {},
   "source": [
    "## Simulate 10 Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb367ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -9.117, steps: 1\n",
      "Episode 2: reward: 661.960, steps: 801\n",
      "Episode 3: reward: 741.877, steps: 923\n",
      "Episode 4: reward: -9.117, steps: 1\n",
      "Episode 5: reward: 595.467, steps: 742\n",
      "Episode 6: reward: 688.663, steps: 816\n",
      "Episode 7: reward: 645.046, steps: 817\n",
      "Episode 8: reward: 726.512, steps: 928\n",
      "Episode 9: reward: -9.117, steps: 1\n",
      "Episode 10: reward: 588.660, steps: 740\n",
      "462.08354696411\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes = 10, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc52270",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c65b69b",
   "metadata": {},
   "source": [
    "### Build Env for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3eba7ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VehEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env_vis \u001b[38;5;241m=\u001b[39m \u001b[43mVehEnv\u001b[49m(\u001b[38;5;28;01mTrue\u001b[39;00m,corridor,mass,frontal_area,air_density,drag_cof,Crr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VehEnv' is not defined"
     ]
    }
   ],
   "source": [
    "env_vis = VehEnv(True,corridor,mass,frontal_area,air_density,drag_cof,Crr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741702c2",
   "metadata": {},
   "source": [
    "### Simulate 1 Run with Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b191fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 715.814, steps: 853\n"
     ]
    }
   ],
   "source": [
    "speeds = []\n",
    "accels = []\n",
    "positions = []\n",
    "test1 = dqn.test(env_vis, nb_episodes = 1, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ddbd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(list(range(len(speeds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5e7dae7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*, block=None)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(1,figsize=(20, 30), dpi=80)\n",
    "\n",
    "plt.subplot(4,1,1)\n",
    "plt.plot(y,accels)\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Acceleration\")\n",
    "\n",
    "plt.subplot(4,1,2)\n",
    "plt.plot(y,speeds)\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "\n",
    "plt.subplot(4,1,3)\n",
    "plt.plot(y,positions)\n",
    "for i in range(0,len(corridor.get_info())):\n",
    "    plt.axhline(corridor.get_info()[i][0],color='purple')\n",
    "for i in range(0,len(positions),10):\n",
    "    for j in range(0,len(light_vis_info[i])):\n",
    "        if light_vis_info[i][j][1] == 1:\n",
    "            plt.scatter(i,light_vis_info[i][j][0],color=\"green\" )\n",
    "        if light_vis_info[i][j][1] == 0:\n",
    "            plt.scatter(i,light_vis_info[i][j][0],color=\"red\" )   \n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Position\")\n",
    "\n",
    "plt.subplot(4,1,4)\n",
    "plt.plot(y,total_energy)\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Energy\")\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3562955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Position')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(y,positions)\n",
    "for i in range(0,len(corridor.get_info())):\n",
    "    plt.axhline(corridor.get_info()[i][0],color='purple')\n",
    "for i in range(0,len(positions),10):\n",
    "    for j in range(0,len(light_vis_info[i])):\n",
    "        if light_vis_info[i][j][1] == 1:\n",
    "            plt.scatter(i,light_vis_info[i][j][0],color=\"green\" )\n",
    "        if light_vis_info[i][j][1] == 0:\n",
    "            plt.scatter(i,light_vis_info[i][j][0],color=\"red\" )   \n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a82e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
